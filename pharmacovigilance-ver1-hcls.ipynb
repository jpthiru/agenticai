{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jpthirumalai/pharmacovigilance-ver1-hcls?scriptVersionId=234251157\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:04:54.866743Z","iopub.execute_input":"2025-04-16T11:04:54.867219Z","iopub.status.idle":"2025-04-16T11:04:55.289661Z","shell.execute_reply.started":"2025-04-16T11:04:54.867173Z","shell.execute_reply":"2025-04-16T11:04:55.288465Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"\"\"\"\nCapstone Project: Multi-Agent System for Real-Time Pharmacovigilance Signal Detection<br>\nDate: 2025-04-08\n\"\"\"","metadata":{}},{"cell_type":"code","source":"\n\n# %% [markdown]\n# # Capstone: Real-Time Pharmacovigilance Signal Detection Agent System\n#\n# **Goal:** Monitor diverse data sources (literature, news, simulated regulatory/social feeds) to identify potential drug safety signals using a multi-agent system powered by Gemini and Vector Search.\n#\n# **Core Components:**\n# 1.  **Data Ingestion:** Fetch/Simulate data from PubMed, News, Social Media, FAERS.\n# 2.  **Embedding & Vector Store:** Embed relevant text data and store in ChromaDB.\n# 3.  **Knowledge Store:** Basic info on drugs, known ADRs (simulated).\n# 4.  **Generative AI Agents:** Specialized agents for scanning, context analysis, and synthesis.\n# 5.  **Agent Communication:** Simple function calls or message passing.\n# 6.  **LLM:** Google Gemini (`gemini-1.5-flash-latest` or `gemini-1.5-pro-latest`).\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:04:55.291132Z","iopub.execute_input":"2025-04-16T11:04:55.291742Z","iopub.status.idle":"2025-04-16T11:04:55.296697Z","shell.execute_reply.started":"2025-04-16T11:04:55.291697Z","shell.execute_reply":"2025-04-16T11:04:55.295385Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:04:55.299006Z","iopub.execute_input":"2025-04-16T11:04:55.299401Z","iopub.status.idle":"2025-04-16T11:05:44.477665Z","shell.execute_reply.started":"2025-04-16T11:04:55.299365Z","shell.execute_reply":"2025-04-16T11:05:44.476412Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.9/454.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install feedparser","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:44.479623Z","iopub.execute_input":"2025-04-16T11:05:44.480056Z","iopub.status.idle":"2025-04-16T11:05:52.001433Z","shell.execute_reply.started":"2025-04-16T11:05:44.48001Z","shell.execute_reply":"2025-04-16T11:05:51.999712Z"}},"outputs":[{"name":"stdout","text":"Collecting feedparser\n  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\nCollecting sgmllib3k (from feedparser)\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=c66c5829b637a59f15f9d21defcbee01a4b15390f854e9a2c764ac329c2d86a5\n  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\nSuccessfully built sgmllib3k\nInstalling collected packages: sgmllib3k, feedparser\nSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nimport google.generativeai as ogenai\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:52.002735Z","iopub.execute_input":"2025-04-16T11:05:52.00323Z","iopub.status.idle":"2025-04-16T11:05:55.642498Z","shell.execute_reply.started":"2025-04-16T11:05:52.00319Z","shell.execute_reply":"2025-04-16T11:05:55.641483Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nCapstone Project: Multi-Agent System for Real-Time Pharmacovigilance Signal Detection\nDate: 2025-04-08\n\"\"\"\n\n# %% [markdown]\n# # Capstone: Real-Time Pharmacovigilance Signal Detection Agent System\n#\n# **Goal:** Monitor diverse data sources (literature, news, simulated regulatory/social feeds) to identify potential drug safety signals using a multi-agent system powered by Gemini and Vector Search.\n#\n# **Core Components:**\n# 1.  **Data Ingestion:** Fetch/Simulate data from PubMed, News, Social Media, FAERS.\n# 2.  **Embedding & Vector Store:** Embed relevant text data and store in ChromaDB.\n# 3.  **Knowledge Store:** Basic info on drugs, known ADRs (simulated).\n# 4.  **Generative AI Agents:** Specialized agents for scanning, context analysis, and synthesis.\n# 5.  **Agent Communication:** Simple function calls or message passing.\n# 6.  **LLM:** Google Gemini (`gemini-1.5-flash-latest` or `gemini-1.5-pro-latest`).\n\n# %%\n# # 1. Setup: Libraries and API Keys\nimport os\nimport json\nimport re\nimport datetime\nimport time\nimport hashlib # For generating consistent IDs\n\n# Core AI/VectorStore Libs\n# import google.generativeai as genai\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\n# Data Source Libs (Install as needed)\nimport requests # For NewsAPI, other web APIs\n# from Bio import Entrez # For PubMed - Requires setup\nimport feedparser # For RSS Feeds (News, some journals)\n# import praw # For Reddit - Requires API setup\n\nprint(f\"Notebook started on: {datetime.datetime.now()}\")\nprint(f\"Current date context: Tuesday, April 8, 2025\") # As per user context\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:55.643651Z","iopub.execute_input":"2025-04-16T11:05:55.644242Z","iopub.status.idle":"2025-04-16T11:05:56.393117Z","shell.execute_reply.started":"2025-04-16T11:05:55.644209Z","shell.execute_reply":"2025-04-16T11:05:56.391928Z"}},"outputs":[{"name":"stdout","text":"Notebook started on: 2025-04-16 11:05:56.389333\nCurrent date context: Tuesday, April 8, 2025\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- Securely load API keys ---\n# Recommended: Use environment variables or Colab secrets\n# Example:\n# GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n# NEWS_API_KEY = os.environ.get(\"NEWS_API_KEY\")\n# REDDIT_CLIENT_ID = os.environ.get(\"REDDIT_CLIENT_ID\")\n# etc.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:56.394212Z","iopub.execute_input":"2025-04-16T11:05:56.394527Z","iopub.status.idle":"2025-04-16T11:05:56.39914Z","shell.execute_reply.started":"2025-04-16T11:05:56.394498Z","shell.execute_reply":"2025-04-16T11:05:56.39748Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\ngoogle_client = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nnews_client = UserSecretsClient().get_secret(\"NEWS_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:56.402654Z","iopub.execute_input":"2025-04-16T11:05:56.402988Z","iopub.status.idle":"2025-04-16T11:05:56.558375Z","shell.execute_reply.started":"2025-04-16T11:05:56.402962Z","shell.execute_reply":"2025-04-16T11:05:56.556931Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"2. Initialize LLM & Embedding Model (Using Google AI)","metadata":{}},{"cell_type":"code","source":"generation_config =[ \n    types.GenerateContentConfig(\n      temperature = 0.7, # Adjust for creativity vs consistency\n      top_p= 1.0,\n      top_k= 32,\n      max_output_tokens= 8192,\n    )\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:56.560335Z","iopub.execute_input":"2025-04-16T11:05:56.560737Z","iopub.status.idle":"2025-04-16T11:05:56.567264Z","shell.execute_reply.started":"2025-04-16T11:05:56.560705Z","shell.execute_reply":"2025-04-16T11:05:56.565823Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Safety settings for Gemini\n# safety_settings = [\n#     {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n#     {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n#     {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n#     {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n# ]\n\nsafety_settings=[\n        types.SafetySetting(\n            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        ),]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:56.568496Z","iopub.execute_input":"2025-04-16T11:05:56.568869Z","iopub.status.idle":"2025-04-16T11:05:56.589697Z","shell.execute_reply.started":"2025-04-16T11:05:56.568837Z","shell.execute_reply":"2025-04-16T11:05:56.58838Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# import google.generativeai as genai2\nclient = None\ntry:\n    client = genai.Client(api_key=google_client)\n    # Using Flash for speed, consider Pro for more complex reasoning\n    model_name=\"gemini-2.0-flash-001\",\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-001\",\n        contents=\"Explain AI to me like I'm a kid.\",\n        # config = generation_config,\n        config = types.GenerateContentConfig(\n            temperature = 0.7, # Adjust for creativity vs consistency\n            top_p= 1.0,\n            top_k= 32,\n            max_output_tokens= 8192,\n            safety_settings=[\n            types.SafetySetting(\n                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n            ),]\n        )\n        # safetySettings = safety_settings\n    )\n    print(response.text)\n    print(f\"Gemini model '{model_name}' initialized.\")\n\n    # Using Google's text embedding model via the Generative AI SDK\n    embedding_model_name = \"models/text-embedding-004\"\n    # Note: Direct embedding function via genai SDK might be simpler for some use cases\n    # Or use ChromaDB's helper with GoogleGenerativeAiEmbeddingFunction if needed\n    google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=google_client, model_name=embedding_model_name)\n    print(f\"Google Embedding model '{embedding_model_name}' ready via ChromaDB helper.\")\n\nexcept Exception as e:\n    print(f\"Error initializing Google AI services: {e}\")\n    # Handle error appropriately (e.g., exit or fallback)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:56.591075Z","iopub.execute_input":"2025-04-16T11:05:56.591531Z","iopub.status.idle":"2025-04-16T11:05:58.842191Z","shell.execute_reply.started":"2025-04-16T11:05:56.5915Z","shell.execute_reply":"2025-04-16T11:05:58.840851Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a really smart toy robot. This robot can do things that usually only people can do, like understand what you say, play games, or even draw pictures!\n\nThat's kind of what AI is! It's like giving computers a brain so they can do smart things.\n\nThink of it like this:\n\n*   **You teach it:** You show the computer lots and lots of examples. Like, if you want it to recognize cats, you show it tons of pictures of cats.\n*   **It learns:** The computer looks at all those examples and figures out what makes a cat a cat.\n*   **It gets smarter:** The more you show it, the better it gets at recognizing cats, even if it's a cat it's never seen before!\n\nSo AI is all about making computers smart enough to learn and do things that usually only people can do. It's used in lots of things, like helping you find videos to watch, answering your questions, and even driving cars! Cool, right?\n\nGemini model '('gemini-2.0-flash-001',)' initialized.\nGoogle Embedding model 'models/text-embedding-004' ready via ChromaDB helper.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Helper function for LLM calls","metadata":{}},{"cell_type":"code","source":"def call_gemini(prompt, llm_model=client, is_json_output=False):\n    \"\"\"Sends a prompt to the Gemini model and returns the text response.\"\"\"\n    try:\n        client = genai.Client(api_key=google_client)\n        # model_name=\"gemini-2.0-flash-001\",\n        # Add instruction for JSON output if requested\n        if is_json_output:\n             prompt += \"\\n\\nPlease format your response strictly as a JSON object.\"\n\n        response = client.models.generate_content(\n            model = \"gemini-2.0-flash\",\n            contents = prompt,\n            # config=generation_config,\n            # safetySettings=safety_settings\n            config = types.GenerateContentConfig(\n                temperature = 0.7, # Adjust for creativity vs consistency\n                top_p= 1.0,\n                top_k= 32,\n                max_output_tokens= 8192,\n                safety_settings=[\n                types.SafetySetting(\n                    category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                    threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n                )]\n            )\n        )\n        # Basic check for blocked content\n        if not response.candidates:\n             print(\"Warning: Response was blocked or empty.\")\n             return None\n        # Attempt to extract text, handle potential errors\n        try:\n            result_text = response.text\n            if is_json_output:\n                # Clean potential markdown ```json ... ```\n                result_text = re.sub(r'^```json\\s*|\\s*```$', '', result_text, flags=re.MULTILINE)\n                return json.loads(result_text) # Parse JSON\n            return result_text\n        except (ValueError, json.JSONDecodeError) as json_err:\n             print(f\"Warning: Could not parse expected JSON output. Error: {json_err}\")\n             print(f\"Raw response: {response.text}\")\n             return None # Or return raw text if preferred fallback\n        except Exception as resp_err:\n            print(f\"Warning: Error extracting text from response. Error: {resp_err}\")\n            return None\n\n    except Exception as e:\n        print(f\"Error calling Gemini API: {e}\")\n        print(traceback.format_exc())\n               \n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:58.843344Z","iopub.execute_input":"2025-04-16T11:05:58.843746Z","iopub.status.idle":"2025-04-16T11:05:58.853358Z","shell.execute_reply.started":"2025-04-16T11:05:58.843711Z","shell.execute_reply":"2025-04-16T11:05:58.851946Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Helper function for Embeddings","metadata":{}},{"cell_type":"code","source":"def get_embedding(text, model_name=embedding_model_name):\n    \"\"\"Generates embeddings for a given text using Google's model.\"\"\"\n    try:\n        result = ogenai.embed_content(model=f\"{model_name}\", content=text, task_type=\"RETRIEVAL_DOCUMENT\")\n        return result['embedding']\n    except Exception as e:\n        print(f\"Error generating embedding: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:58.854624Z","iopub.execute_input":"2025-04-16T11:05:58.855059Z","iopub.status.idle":"2025-04-16T11:05:58.88044Z","shell.execute_reply.started":"2025-04-16T11:05:58.855017Z","shell.execute_reply":"2025-04-16T11:05:58.879282Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"\n## 3. Initialize Vector Store (ChromaDB - Local)","metadata":{}},{"cell_type":"code","source":"\nclient = chromadb.PersistentClient(path=\"./chroma_pv_db\") # Creates directory if not exists\n\n# Using Google Generative AI embeddings with ChromaDB\n# Note: If using google_ef helper, pass it here. Otherwise, generate embeddings separately.\ntry:\n    # Collection for Literature Abstracts/Snippets\n    literature_collection = client.get_or_create_collection(\n        name=\"literature_pv\",\n        embedding_function=google_ef # Use the helper function\n        # metadata={\"hnsw:space\": \"cosine\"} # Optional: Specify distance metric\n    )\n    print(f\"ChromaDB collection 'literature_pv' ready. Item count: {literature_collection.count()}\")\n\n    # Collection for News/Social Media Posts (potentially shorter text)\n    feeds_collection = client.get_or_create_collection(\n        name=\"feeds_pv\",\n        embedding_function=google_ef\n    )\n    print(f\"ChromaDB collection 'feeds_pv' ready. Item count: {feeds_collection.count()}\")\n\nexcept Exception as e:\n    print(f\"Error initializing ChromaDB: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:05:58.881745Z","iopub.execute_input":"2025-04-16T11:05:58.882242Z","iopub.status.idle":"2025-04-16T11:06:00.050504Z","shell.execute_reply.started":"2025-04-16T11:05:58.882197Z","shell.execute_reply":"2025-04-16T11:06:00.049331Z"}},"outputs":[{"name":"stdout","text":"ChromaDB collection 'literature_pv' ready. Item count: 0\nChromaDB collection 'feeds_pv' ready. Item count: 0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Helper function to add to ChromaDB","metadata":{}},{"cell_type":"code","source":"import traceback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.052403Z","iopub.execute_input":"2025-04-16T11:06:00.052807Z","iopub.status.idle":"2025-04-16T11:06:00.058736Z","shell.execute_reply.started":"2025-04-16T11:06:00.052774Z","shell.execute_reply":"2025-04-16T11:06:00.056669Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def add_to_vector_store(collection, documents, metadatas, ids):\n    \"\"\"Adds documents and metadata to the specified ChromaDB collection.\"\"\"\n    if not documents:\n        print(\"No documents to add.\")\n        return\n    try:\n        collection.add(\n            # embeddings=embeddings, # Not needed if embedding_function is set\n            documents=documents,\n            metadatas=metadatas,\n            ids=ids\n        )\n        print(f\"Added {len(documents)} items to collection '{collection.name}'.\")\n    except Exception as e:\n        print(f\"Error adding to ChromaDB collection '{collection.name}': {e}\")\n        print(traceback.format_exc())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.060244Z","iopub.execute_input":"2025-04-16T11:06:00.060663Z","iopub.status.idle":"2025-04-16T11:06:00.081188Z","shell.execute_reply.started":"2025-04-16T11:06:00.06063Z","shell.execute_reply":"2025-04-16T11:06:00.079927Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Helper function to search ChromaDB","metadata":{}},{"cell_type":"code","source":"def search_vector_store(collection, query_text, n_results=5):\n    \"\"\"Searches the collection for text similar to the query text.\"\"\"\n    try:\n        results = collection.query(\n            query_texts=[query_text],\n            n_results=n_results,\n            include=['documents', 'metadatas', 'distances']\n        )\n        return results\n    except Exception as e:\n        print(f\"Error searching ChromaDB collection '{collection.name}': {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.082903Z","iopub.execute_input":"2025-04-16T11:06:00.083358Z","iopub.status.idle":"2025-04-16T11:06:00.106655Z","shell.execute_reply.started":"2025-04-16T11:06:00.083312Z","shell.execute_reply":"2025-04-16T11:06:00.105331Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## 4. Define Knowledge Store (Simulated)\n### In a real system, this could be a database or more structured files.","metadata":{}},{"cell_type":"code","source":"knowledge_store = {\n    \"drugs\": {\n        \"DrugA\": {\"class\": \"ClassX\", \"indication\": \"Indication Y\", \"known_adrs\": [\"Headache\", \"Nausea\"]},\n        \"DrugB\": {\"class\": \"ClassZ\", \"indication\": \"Indication W\", \"known_adrs\": [\"Dizziness\", \"Fatigue\", \"Rash\"]},\n    },\n    \"meddra_mapping\": { # Highly simplified mapping example\n        \"feeling sick\": \"Nausea\",\n        \"stomach ache\": \"Abdominal Pain\",\n        \"spots\": \"Rash\",\n        \"spinning room\": \"Vertigo\",\n        \"tired\": \"Fatigue\",\n        \"head hurts\": \"Headache\"\n    },\n    \"seriousness_keywords\": [\"hospitalized\", \"disability\", \"life-threatening\", \"death\", \"intervention required\"]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.108267Z","iopub.execute_input":"2025-04-16T11:06:00.108734Z","iopub.status.idle":"2025-04-16T11:06:00.125563Z","shell.execute_reply.started":"2025-04-16T11:06:00.108698Z","shell.execute_reply":"2025-04-16T11:06:00.124349Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def get_drug_info(drug_name):\n    return knowledge_store[\"drugs\"].get(drug_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.126789Z","iopub.execute_input":"2025-04-16T11:06:00.127412Z","iopub.status.idle":"2025-04-16T11:06:00.147226Z","shell.execute_reply.started":"2025-04-16T11:06:00.127374Z","shell.execute_reply":"2025-04-16T11:06:00.145589Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def map_to_meddra(term):\n    # Simple keyword matching - real system needs fuzzy matching / NLP model\n    term_lower = term.lower()\n    for key, value in knowledge_store[\"meddra_mapping\"].items():\n        if key in term_lower:\n            return value\n    return term # Return original if no simple map found","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.148364Z","iopub.execute_input":"2025-04-16T11:06:00.148726Z","iopub.status.idle":"2025-04-16T11:06:00.16881Z","shell.execute_reply.started":"2025-04-16T11:06:00.148697Z","shell.execute_reply":"2025-04-16T11:06:00.16729Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def check_seriousness(text):\n    text_lower = text.lower()\n    for keyword in knowledge_store[\"seriousness_keywords\"]:\n        if keyword in text_lower:\n            return True\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.170059Z","iopub.execute_input":"2025-04-16T11:06:00.170506Z","iopub.status.idle":"2025-04-16T11:06:00.194215Z","shell.execute_reply.started":"2025-04-16T11:06:00.170444Z","shell.execute_reply":"2025-04-16T11:06:00.192687Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### --- Agent Function Definitions ---","metadata":{}},{"cell_type":"code","source":"\ndef literature_scanner_agent(drugs_of_interest, search_terms, max_results=10):\n    \"\"\"\n    Monitors PubMed (simulated here) for new relevant literature.\n    Extracts potential ADRs and drug mentions.\n    Adds findings to vector store.\n    Returns list of findings (e.g., AnalyzedItem objects or dicts).\n    \"\"\"\n    print(f\"\\n--- Running Literature Scanner Agent ({datetime.datetime.now()}) ---\")\n    findings = []\n    # TODO: Implement actual PubMed API call using Entrez\n    # Entrez.email = \"Your.Name.Here@example.org\" # Always tell NCBI who you are\n    # handle = Entrez.esearch(db=\"pubmed\", term=\"YourComplexSearchQuery\", retmax=max_results)\n    # record = Entrez.read(handle)\n    # handle.close()\n    # etc... fetch abstracts\n\n    # --- Simulation ---\n    simulated_abstracts = [\n        {\"id\": \"pmid1\", \"text\": \"A study on DrugA found increased reports of severe skin reactions, previously unknown.\", \"drug\": \"DrugA\", \"adr\": \"severe skin reactions\"},\n        {\"id\": \"pmid2\", \"text\": \"DrugB effectiveness was confirmed, common side effects like Fatigue were noted.\", \"drug\": \"DrugB\", \"adr\": \"Fatigue\"},\n        {\"id\": \"pmid3\", \"text\": \"Interesting case report linking DrugA to sudden onset Vertigo.\", \"drug\": \"DrugA\", \"adr\": \"Vertigo\"},\n    ]\n    print(f\"Simulating PubMed search, found {len(simulated_abstracts)} abstracts.\")\n\n    docs_to_embed = []\n    metadatas = []\n    ids = []\n\n    for abstract in simulated_abstracts:\n         # Basic check if drug is relevant\n        if abstract[\"drug\"] in drugs_of_interest:\n            finding = {\n                \"source\": \"pubmed\",\n                \"source_id\": abstract[\"id\"],\n                \"text\": abstract[\"text\"],\n                \"potential_adr\": abstract[\"adr\"],\n                \"drug_mentioned\": abstract[\"drug\"],\n                \"timestamp\": datetime.datetime.now()\n            }\n            findings.append(finding)\n            print(f\"  Found relevant abstract: {abstract['id']}\")\n\n            # Prepare for vector store\n            docs_to_embed.append(abstract[\"text\"])\n            metadatas.append({\n                \"source\": \"pubmed\",\n                \"source_id\": abstract[\"id\"],\n                \"drug\": abstract[\"drug\"],\n                \"adr_mention\": abstract[\"adr\"],\n                \"timestamp\": finding[\"timestamp\"].isoformat()\n            })\n            # Generate a unique, deterministic ID based on content or source ID\n            hash_id = hashlib.sha256(abstract[\"id\"].encode()).hexdigest()\n            ids.append(f\"pubmed_{hash_id}\")\n\n    embedded_docs=get_embedding(docs_to_embed)\n    # Add findings to Vector Store\n    if docs_to_embed:\n        add_to_vector_store(literature_collection, documents=docs_to_embed, metadatas=metadatas, ids=ids)\n\n    return findings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.195414Z","iopub.execute_input":"2025-04-16T11:06:00.195839Z","iopub.status.idle":"2025-04-16T11:06:00.216641Z","shell.execute_reply.started":"2025-04-16T11:06:00.195786Z","shell.execute_reply":"2025-04-16T11:06:00.215301Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def news_listener_agent(drugs_of_interest, keywords, max_results=20):\n    \"\"\"\n    Monitors NewsAPI (or RSS) for relevant articles.\n    Uses LLM to extract potential ADRs and drug mentions.\n    Adds findings to vector store.\n    Returns list of findings.\n    \"\"\"\n    try:\n        print(f\"\\n--- Running News Listener Agent ({datetime.datetime.now()}) ---\")\n        findings = []\n        # --- Actual NewsAPI Call ---\n        # url = f\"https://newsapi.org/v2/everything?q={'+OR+'.join(keywords)}&apiKey={NEWS_API_KEY}&pageSize={max_results}&sortBy=publishedAt\"\n        # try:\n        #     response = requests.get(url)\n        #     response.raise_for_status() # Raise HTTPError for bad responses (4XX, 5XX)\n        #     articles = response.json().get('articles', [])\n        # except requests.exceptions.RequestException as e:\n        #     print(f\"Error fetching news: {e}\")\n        #     articles = []\n    \n        # --- Simulation ---\n        articles = [\n            {\"source\": {\"name\": \"HealthNews\"}, \"title\": \"Concerns grow over DrugA side effects\", \"description\": \"Patients report unexpected issues like Vertigo after taking DrugA.\", \"url\": \"http://example.com/news1\", \"publishedAt\": datetime.datetime.now().isoformat()},\n            {\"source\": {\"name\": \"Tech Chronicle\"}, \"title\": \"New AI for Drug Discovery\", \"description\": \"Mentions DrugC development.\", \"url\": \"http://example.com/news2\", \"publishedAt\": datetime.datetime.now().isoformat()},\n            {\"source\": {\"name\": \"Med Journal\"}, \"title\": \"DrugB trial results positive\", \"description\": \"Standard side effects noted, effectiveness confirmed.\", \"url\": \"http://example.com/news3\", \"publishedAt\": datetime.datetime.now().isoformat()},\n        ]\n        print(f\"Simulating News search, found {len(articles)} articles.\")\n    \n        docs_to_embed = []\n        metadatas = []\n        ids = []\n    \n        for article in articles:\n            content_to_analyze = f\"Title: {article.get('title', '')}\\nDescription: {article.get('description', '')}\"\n    \n            # Use LLM to check relevance and extract info\n            prompt = f\"\"\"Analyze the following news snippet. Does it mention any specific drugs from the list [{', '.join(drugs_of_interest)}]? Does it mention any potential adverse drug reactions or side effects?\n    \n            News Snippet:\n            \"{content_to_analyze}\"\n    \n            If it mentions both a relevant drug AND a potential side effect, respond in JSON format with keys \"relevant\": true, \"drug_mentioned\": [\"list of drugs\"], \"potential_adr\": [\"list of adrs\"].\n            Otherwise, respond with \"relevant\": false.\n            \"\"\"\n            llm_response = call_gemini(prompt, is_json_output=True)\n    \n            if llm_response and llm_response.get(\"relevant\"):\n                drug = llm_response.get(\"drug_mentioned\", [None])[0] # Take first mentioned relevant drug\n                adr = llm_response.get(\"potential_adr\", [None])[0] # Take first mentioned relevant adr\n    \n                if drug in drugs_of_interest and adr:\n                    finding = {\n                        \"source\": \"news\",\n                        \"source_id\": article.get('url', f\"news_{hashlib.sha256(content_to_analyze.encode()).hexdigest()}\"),\n                        \"text\": content_to_analyze,\n                        \"potential_adr\": adr,\n                        \"drug_mentioned\": drug,\n                        \"timestamp\": datetime.datetime.fromisoformat(article.get('publishedAt').replace(\"Z\", \"+00:00\")) if article.get('publishedAt') else datetime.datetime.now()\n                    }\n                    findings.append(finding)\n                    print(f\"  Relevant news item found: {article.get('url')}\")\n    \n                    # Prepare for vector store\n                    docs_to_embed.append(content_to_analyze)\n                    metadatas.append({\n                        \"source\": \"news\",\n                        \"source_id\": finding[\"source_id\"],\n                        \"drug\": drug,\n                        \"adr_mention\": adr,\n                        \"timestamp\": finding[\"timestamp\"].isoformat()\n                    })\n                    ids.append(f\"news_{hashlib.sha256(finding['source_id'].encode()).hexdigest()}\")\n    \n    \n        # Add findings to Vector Store\n        if docs_to_embed:\n            add_to_vector_store(feeds_collection, documents=docs_to_embed, metadatas=metadatas, ids=ids)\n    \n        return findings\n    except Exception as e:\n        print(traceback.format_exc())\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.221288Z","iopub.execute_input":"2025-04-16T11:06:00.221696Z","iopub.status.idle":"2025-04-16T11:06:00.245165Z","shell.execute_reply.started":"2025-04-16T11:06:00.221662Z","shell.execute_reply":"2025-04-16T11:06:00.243713Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### TODO: Add similar agents for Social Media (Reddit/PRAW), simulated FAERS data","metadata":{}},{"cell_type":"code","source":"def clinical_context_agent(items):\n    \"\"\"\n    Analyzes findings from other agents.\n    Standardizes ADR terms using Knowledge Store (MedDRA map).\n    Checks if ADR is known for the drug.\n    Assesses potential seriousness.\n    Returns list of contextualized findings.\n    \"\"\"\n    print(f\"\\n--- Running Clinical Context Agent ({datetime.datetime.now()}) ---\")\n    contextualized_findings = []\n    for item in items:\n        drug = item['drug_mentioned']\n        adr_raw = item['potential_adr']\n\n        # Standardize ADR term\n        adr_standardized = map_to_meddra(adr_raw)\n\n        # Check if known ADR for this drug\n        drug_info = get_drug_info(drug)\n        is_known_adr = False\n        if drug_info:\n            # Basic check - real system might need fuzzy matching\n            is_known_adr = any(known.lower() in adr_standardized.lower() for known in drug_info['known_adrs'])\n\n        # Check seriousness\n        is_serious = check_seriousness(item['text']) or check_seriousness(adr_raw)\n\n        # Add context to the finding\n        item['adr_standardized'] = adr_standardized\n        item['is_known_adr'] = is_known_adr\n        item['is_serious'] = is_serious\n        contextualized_findings.append(item)\n        print(f\"  Contextualized: {item['source_id']} - ADR: {adr_standardized} (Known: {is_known_adr}, Serious: {is_serious})\")\n\n    return contextualized_findings\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.246785Z","iopub.execute_input":"2025-04-16T11:06:00.247148Z","iopub.status.idle":"2025-04-16T11:06:00.268985Z","shell.execute_reply.started":"2025-04-16T11:06:00.247119Z","shell.execute_reply":"2025-04-16T11:06:00.267842Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def signal_synthesizer_agent(contextualized_items, time_window_days=7, min_reports_for_signal=3):\n    \"\"\"\n    Looks for patterns and correlations in contextualized findings.\n    Flags potential signals based on criteria (e.g., multiple reports of unexpected ADR).\n    Uses Vector Store to find similar past reports.\n    Returns list of potential signals (e.g., Signal objects or dicts).\n    \"\"\"\n    print(f\"\\n--- Running Signal Synthesizer Agent ({datetime.datetime.now()}) ---\")\n    potential_signals = []\n    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=time_window_days)\n\n    # Group findings by Drug and Standardized ADR within the time window\n    adr_groups = {}\n    for item in contextualized_items:\n        # Ensure timestamp is timezone-aware or convert naive to aware for comparison\n        item_ts = item['timestamp']\n        if item_ts.tzinfo is None:\n             # Assuming UTC if naive, adjust as needed based on source data timezone\n             item_ts = item_ts.replace(tzinfo=datetime.timezone.utc)\n\n        if item_ts < cutoff_date.replace(tzinfo=datetime.timezone.utc): # Make cutoff aware too\n            continue\n\n        key = (item['drug_mentioned'], item['adr_standardized'])\n        if key not in adr_groups:\n            adr_groups[key] = []\n        adr_groups[key].append(item)\n\n    # Analyze groups\n    for (drug, adr), items in adr_groups.items():\n        # Basic Signal Criteria: Multiple reports of an *unexpected* and potentially *serious* ADR\n        num_reports = len(items)\n        is_unexpected = not items[0]['is_known_adr'] # Assumes consistency within group\n        has_serious_report = any(item['is_serious'] for item in items)\n\n        # Example Rule: Signal if >= min_reports of an unexpected ADR, OR if >= N reports of a serious ADR (even if known)\n        if (is_unexpected and num_reports >= min_reports_for_signal) or \\\n           (has_serious_report and num_reports >= min_reports_for_signal + 2): # Stricter threshold for serious\n\n            # Use Vector Store to find related historical items (optional enhancement)\n            # query = f\"Reports related to {drug} and {adr}\"\n            # similar_historical_reports = search_vector_store(literature_collection, query, n_results=5)\n            # print(f\"Found similar historical reports: {similar_historical_reports}\")\n\n            # Generate Signal\n            evidence_summary = f\"Found {num_reports} reports of '{adr}' for {drug} within the last {time_window_days} days. \"\n            evidence_summary += f\"This ADR is considered {'unexpected' if is_unexpected else 'known'}. \"\n            if has_serious_report:\n                evidence_summary += \"At least one report mentioned serious outcomes. \"\n            source_ids = [item['source_id'] for item in items]\n\n            signal = {\n                \"drugs\": [drug],\n                \"adr_term\": adr,\n                \"evidence\": evidence_summary,\n                \"sources\": source_ids,\n                \"confidence_score\": 0.6 + min(0.4, (num_reports / (min_reports_for_signal * 2))), # Simple confidence heuristic\n                \"timestamp\": datetime.datetime.now()\n            }\n            potential_signals.append(signal)\n            print(f\"  Potential Signal Identified: {drug} - {adr}\")\n            print(f\"    Evidence: {evidence_summary}\")\n\n    return potential_signals\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.270184Z","iopub.execute_input":"2025-04-16T11:06:00.270513Z","iopub.status.idle":"2025-04-16T11:06:00.29905Z","shell.execute_reply.started":"2025-04-16T11:06:00.270477Z","shell.execute_reply":"2025-04-16T11:06:00.297722Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"\n### 6. Main Workflow Orchestration\n","metadata":{}},{"cell_type":"code","source":"DRUGS_TO_MONITOR = [\"DrugA\", \"DrugB\"] # Example list\nKEYWORDS_FOR_NEWS = DRUGS_TO_MONITOR + [\"side effect\", \"adverse reaction\"] # Example keywords\nKEYWORDS_FOR_LITERATURE = [f\"{drug}[Title/Abstract] AND (adverse event[MeSH Terms] OR side effect[Title/Abstract])\" for drug in DRUGS_TO_MONITOR] # Example PubMed query parts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.299922Z","iopub.execute_input":"2025-04-16T11:06:00.30026Z","iopub.status.idle":"2025-04-16T11:06:00.328469Z","shell.execute_reply.started":"2025-04-16T11:06:00.300221Z","shell.execute_reply":"2025-04-16T11:06:00.327277Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### --- Run Agents Sequentially (Simple Orchestration) ---\n### In a real system, this could run on a schedule (e.g., daily, hourly)","metadata":{}},{"cell_type":"markdown","source":"#### 1. Data Ingestion / Scanning","metadata":{}},{"cell_type":"code","source":"lit_findings = literature_scanner_agent(DRUGS_TO_MONITOR, KEYWORDS_FOR_LITERATURE)\n# news_findings = news_listener_agent(DRUGS_TO_MONITOR, KEYWORDS_FOR_NEWS)\n# social_findings = social_listener_agent(...) # TODO\n# faers_findings = faers_processor_agent(...) # TODO\nall_raw_findings = lit_findings #+ news_findings # + social_findings + faers_findings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:00.329631Z","iopub.execute_input":"2025-04-16T11:06:00.329991Z","iopub.status.idle":"2025-04-16T11:06:01.57208Z","shell.execute_reply.started":"2025-04-16T11:06:00.32996Z","shell.execute_reply":"2025-04-16T11:06:01.570954Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Literature Scanner Agent (2025-04-16 11:06:00.346211) ---\nSimulating PubMed search, found 3 abstracts.\n  Found relevant abstract: pmid1\n  Found relevant abstract: pmid2\n  Found relevant abstract: pmid3\nAdded 3 items to collection 'literature_pv'.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"news_findings = news_listener_agent(DRUGS_TO_MONITOR, KEYWORDS_FOR_NEWS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:01.573223Z","iopub.execute_input":"2025-04-16T11:06:01.573661Z","iopub.status.idle":"2025-04-16T11:06:04.604572Z","shell.execute_reply.started":"2025-04-16T11:06:01.573622Z","shell.execute_reply":"2025-04-16T11:06:04.602592Z"}},"outputs":[{"name":"stdout","text":"\n--- Running News Listener Agent (2025-04-16 11:06:01.574878) ---\nSimulating News search, found 3 articles.\n  Relevant news item found: http://example.com/news1\n  Relevant news item found: http://example.com/news3\nAdded 2 items to collection 'feeds_pv'.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"all_raw_findings = lit_findings + news_findings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:04.606052Z","iopub.execute_input":"2025-04-16T11:06:04.606432Z","iopub.status.idle":"2025-04-16T11:06:04.611177Z","shell.execute_reply.started":"2025-04-16T11:06:04.606406Z","shell.execute_reply":"2025-04-16T11:06:04.610037Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(all_raw_findings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:04.612327Z","iopub.execute_input":"2025-04-16T11:06:04.612818Z","iopub.status.idle":"2025-04-16T11:06:04.637817Z","shell.execute_reply.started":"2025-04-16T11:06:04.612775Z","shell.execute_reply":"2025-04-16T11:06:04.636226Z"}},"outputs":[{"name":"stdout","text":"[{'source': 'pubmed', 'source_id': 'pmid1', 'text': 'A study on DrugA found increased reports of severe skin reactions, previously unknown.', 'potential_adr': 'severe skin reactions', 'drug_mentioned': 'DrugA', 'timestamp': datetime.datetime(2025, 4, 16, 11, 6, 0, 347486)}, {'source': 'pubmed', 'source_id': 'pmid2', 'text': 'DrugB effectiveness was confirmed, common side effects like Fatigue were noted.', 'potential_adr': 'Fatigue', 'drug_mentioned': 'DrugB', 'timestamp': datetime.datetime(2025, 4, 16, 11, 6, 0, 347576)}, {'source': 'pubmed', 'source_id': 'pmid3', 'text': 'Interesting case report linking DrugA to sudden onset Vertigo.', 'potential_adr': 'Vertigo', 'drug_mentioned': 'DrugA', 'timestamp': datetime.datetime(2025, 4, 16, 11, 6, 0, 347614)}, {'source': 'news', 'source_id': 'http://example.com/news1', 'text': 'Title: Concerns grow over DrugA side effects\\nDescription: Patients report unexpected issues like Vertigo after taking DrugA.', 'potential_adr': 'Vertigo', 'drug_mentioned': 'DrugA', 'timestamp': datetime.datetime(2025, 4, 16, 11, 6, 1, 574957)}, {'source': 'news', 'source_id': 'http://example.com/news3', 'text': 'Title: DrugB trial results positive\\nDescription: Standard side effects noted, effectiveness confirmed.', 'potential_adr': 'Standard side effects', 'drug_mentioned': 'DrugB', 'timestamp': datetime.datetime(2025, 4, 16, 11, 6, 1, 574967)}]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"### TODO: Add similar agents for Social Media (Reddit/PRAW), simulated FAERS data\n","metadata":{}},{"cell_type":"code","source":"def clinical_context_agent(items):\n    \"\"\"\n    Analyzes findings from other agents.\n    Standardizes ADR terms using Knowledge Store (MedDRA map).\n    Checks if ADR is known for the drug.\n    Assesses potential seriousness.\n    Returns list of contextualized findings.\n    \"\"\"\n    print(f\"\\n--- Running Clinical Context Agent ({datetime.datetime.now()}) ---\")\n    contextualized_findings = []\n    for item in items:\n        drug = item['drug_mentioned']\n        adr_raw = item['potential_adr']\n\n        # Standardize ADR term\n        adr_standardized = map_to_meddra(adr_raw)\n\n        # Check if known ADR for this drug\n        drug_info = get_drug_info(drug)\n        is_known_adr = False\n        if drug_info:\n            # Basic check - real system might need fuzzy matching\n            is_known_adr = any(known.lower() in adr_standardized.lower() for known in drug_info['known_adrs'])\n\n        # Check seriousness\n        is_serious = check_seriousness(item['text']) or check_seriousness(adr_raw)\n\n        # Add context to the finding\n        item['adr_standardized'] = adr_standardized\n        item['is_known_adr'] = is_known_adr\n        item['is_serious'] = is_serious\n        contextualized_findings.append(item)\n        print(f\"  Contextualized: {item['source_id']} - ADR: {adr_standardized} (Known: {is_known_adr}, Serious: {is_serious})\")\n\n    return contextualized_findings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:04.639629Z","iopub.execute_input":"2025-04-16T11:06:04.639989Z","iopub.status.idle":"2025-04-16T11:06:04.661297Z","shell.execute_reply.started":"2025-04-16T11:06:04.639964Z","shell.execute_reply":"2025-04-16T11:06:04.660212Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def signal_synthesizer_agent(contextualized_items, time_window_days=7, min_reports_for_signal=3):\n    \"\"\"\n    Looks for patterns and correlations in contextualized findings.\n    Flags potential signals based on criteria (e.g., multiple reports of unexpected ADR).\n    Uses Vector Store to find similar past reports.\n    Returns list of potential signals (e.g., Signal objects or dicts).\n    \"\"\"\n    print(f\"\\n--- Running Signal Synthesizer Agent ({datetime.datetime.now()}) ---\")\n    potential_signals = []\n    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=time_window_days)\n\n    # Group findings by Drug and Standardized ADR within the time window\n    adr_groups = {}\n    for item in contextualized_items:\n        # Ensure timestamp is timezone-aware or convert naive to aware for comparison\n        item_ts = item['timestamp']\n        if item_ts.tzinfo is None:\n             # Assuming UTC if naive, adjust as needed based on source data timezone\n             item_ts = item_ts.replace(tzinfo=datetime.timezone.utc)\n\n        if item_ts < cutoff_date.replace(tzinfo=datetime.timezone.utc): # Make cutoff aware too\n            continue\n\n        key = (item['drug_mentioned'], item['adr_standardized'])\n        if key not in adr_groups:\n            adr_groups[key] = []\n        adr_groups[key].append(item)\n\n    # Analyze groups\n    for (drug, adr), items in adr_groups.items():\n        # Basic Signal Criteria: Multiple reports of an *unexpected* and potentially *serious* ADR\n        num_reports = len(items)\n        is_unexpected = not items[0]['is_known_adr'] # Assumes consistency within group\n        has_serious_report = any(item['is_serious'] for item in items)\n\n        # Example Rule: Signal if >= min_reports of an unexpected ADR, OR if >= N reports of a serious ADR (even if known)\n        if (is_unexpected and num_reports >= min_reports_for_signal) or \\\n           (has_serious_report and num_reports >= min_reports_for_signal + 2): # Stricter threshold for serious\n\n            # Use Vector Store to find related historical items (optional enhancement)\n            # query = f\"Reports related to {drug} and {adr}\"\n            # similar_historical_reports = search_vector_store(literature_collection, query, n_results=5)\n            # print(f\"Found similar historical reports: {similar_historical_reports}\")\n\n            # Generate Signal\n            evidence_summary = f\"Found {num_reports} reports of '{adr}' for {drug} within the last {time_window_days} days. \"\n            evidence_summary += f\"This ADR is considered {'unexpected' if is_unexpected else 'known'}. \"\n            if has_serious_report:\n                evidence_summary += \"At least one report mentioned serious outcomes. \"\n            source_ids = [item['source_id'] for item in items]\n\n            signal = {\n                \"drugs\": [drug],\n                \"adr_term\": adr,\n                \"evidence\": evidence_summary,\n                \"sources\": source_ids,\n                \"confidence_score\": 0.6 + min(0.4, (num_reports / (min_reports_for_signal * 2))), # Simple confidence heuristic\n                \"timestamp\": datetime.datetime.now()\n            }\n            potential_signals.append(signal)\n            print(f\"  Potential Signal Identified: {drug} - {adr}\")\n            print(f\"    Evidence: {evidence_summary}\")\n\n    return potential_signals","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:04.662518Z","iopub.execute_input":"2025-04-16T11:06:04.663158Z","iopub.status.idle":"2025-04-16T11:06:04.687127Z","shell.execute_reply.started":"2025-04-16T11:06:04.663115Z","shell.execute_reply":"2025-04-16T11:06:04.685718Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### 6. Main Workflow Orchestration","metadata":{}},{"cell_type":"code","source":"DRUGS_TO_MONITOR = [\"DrugA\", \"DrugB\"] # Example list\nKEYWORDS_FOR_NEWS = DRUGS_TO_MONITOR + [\"side effect\", \"adverse reaction\"] # Example keywords\nKEYWORDS_FOR_LITERATURE = [f\"{drug}[Title/Abstract] AND (adverse event[MeSH Terms] OR side effect[Title/Abstract])\" for drug in DRUGS_TO_MONITOR] # Example PubMed query parts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:06:04.688497Z","iopub.execute_input":"2025-04-16T11:06:04.688869Z","iopub.status.idle":"2025-04-16T11:06:04.720686Z","shell.execute_reply.started":"2025-04-16T11:06:04.688827Z","shell.execute_reply":"2025-04-16T11:06:04.719294Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### --- Run Agents Sequentially (Simple Orchestration) ---\n### In a real system, this could run on a schedule (e.g., daily, hourly)\n\n### 1. Data Ingestion / Scanning","metadata":{}},{"cell_type":"code","source":"lit_findings = literature_scanner_agent(DRUGS_TO_MONITOR, KEYWORDS_FOR_LITERATURE)\nnews_findings = news_listener_agent(DRUGS_TO_MONITOR, KEYWORDS_FOR_NEWS)\n# social_findings = social_listener_agent(...) # TODO\n# faers_findings = faers_processor_agent(...) # TODO\n\nall_raw_findings = lit_findings + news_findings # + social_findings + faers_findings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:07:34.093161Z","iopub.execute_input":"2025-04-16T11:07:34.09365Z","iopub.status.idle":"2025-04-16T11:07:37.890959Z","shell.execute_reply.started":"2025-04-16T11:07:34.093613Z","shell.execute_reply":"2025-04-16T11:07:37.889707Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Literature Scanner Agent (2025-04-16 11:07:34.094806) ---\nSimulating PubMed search, found 3 abstracts.\n  Found relevant abstract: pmid1\n  Found relevant abstract: pmid2\n  Found relevant abstract: pmid3\nAdded 3 items to collection 'literature_pv'.\n\n--- Running News Listener Agent (2025-04-16 11:07:34.943071) ---\nSimulating News search, found 3 articles.\n  Relevant news item found: http://example.com/news1\n  Relevant news item found: http://example.com/news3\nAdded 2 items to collection 'feeds_pv'.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# 2. Contextualization\ncontextualized_findings = clinical_context_agent(all_raw_findings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:07:48.886531Z","iopub.execute_input":"2025-04-16T11:07:48.886946Z","iopub.status.idle":"2025-04-16T11:07:48.894152Z","shell.execute_reply.started":"2025-04-16T11:07:48.886915Z","shell.execute_reply":"2025-04-16T11:07:48.892942Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Clinical Context Agent (2025-04-16 11:07:48.887763) ---\n  Contextualized: pmid1 - ADR: severe skin reactions (Known: False, Serious: False)\n  Contextualized: pmid2 - ADR: Fatigue (Known: True, Serious: False)\n  Contextualized: pmid3 - ADR: Vertigo (Known: False, Serious: False)\n  Contextualized: http://example.com/news1 - ADR: Vertigo (Known: False, Serious: False)\n  Contextualized: http://example.com/news3 - ADR: Standard side effects (Known: False, Serious: False)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# 3. Synthesis / Signal Detection\ndetected_signals = signal_synthesizer_agent(contextualized_findings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:08:02.915914Z","iopub.execute_input":"2025-04-16T11:08:02.916287Z","iopub.status.idle":"2025-04-16T11:08:02.921526Z","shell.execute_reply.started":"2025-04-16T11:08:02.916253Z","shell.execute_reply":"2025-04-16T11:08:02.920327Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Signal Synthesizer Agent (2025-04-16 11:08:02.917390) ---\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# # 7. Output / Demonstration\n\nprint(\"\\n\\n--- Final Detected Signals ---\")\nif detected_signals:\n    for i, signal in enumerate(detected_signals):\n        print(f\"\\nSignal {i+1}:\")\n        print(f\"  Drug(s): {', '.join(signal['drugs'])}\")\n        print(f\"  ADR Term: {signal['adr_term']}\")\n        print(f\"  Evidence: {signal['evidence']}\")\n        print(f\"  Confidence: {signal['confidence_score']:.2f}\")\n        print(f\"  Supporting Sources: {', '.join(signal['sources'])}\")\n        print(f\"  Timestamp: {signal['timestamp']}\")\nelse:\n    print(\"No significant signals detected in this run.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:08:23.54417Z","iopub.execute_input":"2025-04-16T11:08:23.544569Z","iopub.status.idle":"2025-04-16T11:08:23.552635Z","shell.execute_reply.started":"2025-04-16T11:08:23.544516Z","shell.execute_reply":"2025-04-16T11:08:23.551204Z"}},"outputs":[{"name":"stdout","text":"\n\n--- Final Detected Signals ---\nNo significant signals detected in this run.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}